{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DTree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def train(self, X, y,feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        classes = list(set(y))\n",
    "        feature_names = self.feature_names\n",
    "        return self.build_tree(X, y, classes, feature_names)\n",
    "    \n",
    "    def build_tree(self, X, y, classes, feature_names, maxlevel=-1,level=0,forest=0):\n",
    "        \n",
    "        # it makes nodes and add subtrees recursively\n",
    "        \n",
    "        n_data = len(X)\n",
    "        n_features = len(feature_names)\n",
    "        \n",
    "        # compute total entropy and gini df.groupby(['Activity']).size()\n",
    "        class_frequency = np.zeros(len(classes))\n",
    "        total_entropy = 0\n",
    "        total_gini = 0\n",
    "        index = 0\n",
    "        \n",
    "        for a_class in classes:\n",
    "            class_frequency[index] = y.count(a_class)\n",
    "            total_entropy += self.cal_entropy(float(class_frequency[index]/n_data))\n",
    "            total_gini += (float(class_frequency[index]/n_data))**2\n",
    "            index += 1\n",
    "        \n",
    "        total_gini = 1 - total_gini\n",
    "        default_best_feature = classes[np.argmax(class_frequency)]\n",
    "        \n",
    "        # base case\n",
    "        if (n_data == 0 or n_features == 0 or (maxlevel>=0 and level>maxlevel)):\n",
    "            return default_best_feature\n",
    "        elif len(classes) == 1:\n",
    "            return classes[0]\n",
    "        else:\n",
    "            # calculate which feature yeilds highest info gain\n",
    "            info_gain = np.zeros(n_features)\n",
    "            gini_gain = np.zeros(n_features)\n",
    "            feature_set = range(n_features)\n",
    "            if (forest != 0):\n",
    "                np.random.shuffle(feature_set)\n",
    "                feature_set = feature_set[0:forest]\n",
    "\n",
    "            for feature in feature_set:\n",
    "                entropy, gini = self.cal_info_gini_gain(X, y, classes, feature)\n",
    "                info_gain[feature] = total_entropy - entropy\n",
    "                gini_gain[feature] = total_gini - gini\n",
    "                print(feature_names[feature], \" entropy is: \", info_gain[feature])\n",
    "            \n",
    "            best_feature = np.argmax(info_gain)\n",
    "            tree = {feature_names[best_feature]:{}}\n",
    "            print('best_feature is:',feature_names[best_feature])\n",
    "\n",
    "            # build a tree based on the best feature node\n",
    "            # by adding a subtrees to each of possible value\n",
    "            # of the best feature. When adding subtrees we need\n",
    "            # to use data and features where the best feature is excluded\n",
    "            values = []\n",
    "            for a_x in X:\n",
    "                if a_x[best_feature] not in values:\n",
    "                    values.append(a_x[best_feature])\n",
    "\n",
    "            for value in values:\n",
    "                new_X = []\n",
    "                new_y = y\n",
    "                index = 0\n",
    "                all_y = []\n",
    "\n",
    "                for a_x in X:\n",
    "                    if a_x[best_feature] == value:\n",
    "                        if best_feature == 0:\n",
    "                            new_a_x = a_x[1:]\n",
    "                            new_feature_names = feature_names[1:]\n",
    "                        elif best_feature == n_features:\n",
    "                            new_a_x = a_x[:-1]\n",
    "                            new_feature_names = feature_names[:-1]\n",
    "                        else:\n",
    "                            new_a_x = a_x[:best_feature] + a_x[best_feature+1:]\n",
    "                            new_feature_names = feature_names[:best_feature] + feature_names[best_feature+1:]\n",
    "                        all_y.append(y[index])\n",
    "                        new_X.append(new_a_x)\n",
    "                    index += 1\n",
    "                    \n",
    "                \n",
    "                new_classes = list(set(all_y))\n",
    "                subtree = self.build_tree(new_X, all_y, new_classes, new_feature_names, maxlevel, level+1, forest)\n",
    "                \n",
    "                tree[feature_names[best_feature]][value] = subtree\n",
    "            \n",
    "            return tree\n",
    "        \n",
    "    def cal_entropy(self, prob):\n",
    "        if prob != 0:\n",
    "            return -prob * np.log2(prob)\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def cal_info_gini_gain(self, X, y, classes, feature):\n",
    "        # calculate the info gain and gini\n",
    "        n_data = len(X)\n",
    "        entropy = 0\n",
    "        gini_gain = 0\n",
    "\n",
    "        unique_values = []\n",
    "        for a_x in X:\n",
    "            if a_x[feature] not in unique_values:\n",
    "                unique_values.append(a_x[feature])\n",
    "        \n",
    "        # Tested\n",
    "        #print('unique_values',unique_values)\n",
    "\n",
    "        for value in unique_values:\n",
    "            # count the number of feature value f with its # of different classes\n",
    "            value_cnt = 0\n",
    "            class_cnt = Counter()\n",
    "            index = 0\n",
    "            n_classes = 0\n",
    "            for a_x in X:\n",
    "                if a_x[feature] == value:\n",
    "                    value_cnt += 1\n",
    "                    n_classes += 1\n",
    "                    class_cnt[y[index]] += 1\n",
    "                index += 1\n",
    "            \n",
    "            feature_prob = float(value_cnt/n_data)\n",
    "\n",
    "            class_entropy = 0\n",
    "            gini = 0\n",
    "            for ind_class in class_cnt:\n",
    "                class_prob = float(class_cnt[ind_class]/n_classes)\n",
    "                gini += class_prob**2\n",
    "                class_entropy += self.cal_entropy(class_prob)\n",
    "\n",
    "            entropy += feature_prob * class_entropy\n",
    "            gini_gain += gini * feature_prob\n",
    "\n",
    "        return entropy, 1 - gini_gain\n",
    "    \n",
    "    def predict(self, tree, data):\n",
    "        if type(tree) == type(\"string\"):\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.feature_names)):\n",
    "                if self.feature_names[i] == a:\n",
    "                    break\n",
    "            try:\n",
    "                t = tree[a][data[i]]\n",
    "                return self.predict(t, data)\n",
    "            except:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age  entropy is:  0.45143261817650915\n",
      "Education  entropy is:  0.14354652412251911\n",
      "Income  entropy is:  0.03030514483932223\n",
      "Marital Status  entropy is:  0.1024101186092028\n",
      "best_feature is: Age\n",
      "Education  entropy is:  0.25322894362036363\n",
      "Income  entropy is:  0.0032289436203635224\n",
      "Marital Status  entropy is:  0.954434002924965\n",
      "best_feature is: Marital Status\n",
      "Education  entropy is:  0.0\n",
      "Income  entropy is:  0.9182958340544896\n",
      "Marital Status  entropy is:  0.2516291673878229\n",
      "best_feature is: Income\n",
      "\n",
      "The tree is:  {\n",
      " \"Age\": {\n",
      "  \"36 - 55\": {\n",
      "   \"Marital Status\": {\n",
      "    \"single\": \"will buy\",\n",
      "    \"married\": \"won't buy\"\n",
      "   }\n",
      "  },\n",
      "  \"18 - 35\": \"won't buy\",\n",
      "  \"< 18\": {\n",
      "   \"Income\": {\n",
      "    \"low\": \"will buy\",\n",
      "    \"high\": \"won't buy\"\n",
      "   }\n",
      "  },\n",
      "  \"> 55\": \"will buy\"\n",
      " }\n",
      "}\n",
      "\n",
      "test accuracy is: 1.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('discrete-training.csv',encoding='utf-8-sig')\n",
    "train_X = data.drop(labels=['will_buy?'], axis=1).values.tolist()\n",
    "feature_names = ['Age','Education','Income','Marital Status']\n",
    "train_y = data['will_buy?'].values.tolist()\n",
    "\n",
    "dtree = DTree()\n",
    "tree = dtree.train(X, y,feature_names=feature_names)\n",
    "\n",
    "print()\n",
    "print('The tree is: ',json.dumps(tree, indent=1))\n",
    "print()\n",
    "\n",
    "test_data = pd.read_csv('discrete-test.csv',encoding='utf-8-sig')\n",
    "test_X = test_data.drop(labels=['will_buy?'], axis=1).values.tolist()\n",
    "test_y = test_data['will_buy?'].values.tolist()\n",
    "\n",
    "total_correct = 0\n",
    "for i in range(len(test_X)):\n",
    "    prediction = dtree.predict(tree,test_X[i])\n",
    "    if test_y[i] == prediction:\n",
    "        total_correct += 1\n",
    "        \n",
    "print('test accuracy is:',total_correct/len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
